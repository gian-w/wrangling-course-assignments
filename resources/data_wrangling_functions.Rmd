---
title: "Important functions for data wrangling"
author: "Template: Ian Hussey; Content: Gian Wegmueller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}

# set knit options
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

# disable scientific notation
options(scipen = 999) 

```

# Instructions

Most data wrangling tasks can be accomplished with a relatively small number of functions. I've listed the most important ones here. 

Use this file to keep notes about what a given function does in your own words, the situations where you would use it, and working examples. You can make use of built-in datasets to do this (e.g., `mtcars`) or load external data sets (although its easier to break the examples if you modify the data etc.).

Learning how to wrangle data efficiently involves a combination of knowing how to break the problem down into smaller components; knowing which functions are available to accomplish each component operation and how to use them; how to search documentation to learn or refresh your knowledge of how a function works; and how to discover new functions that accomplish new component operations. 

Remember that you can look up the help documentation for any function by typing a question mark followed by its name in the console, e.g., `?dplyr::mutate`. The help documentation provides details of a function's parameters and defaults, its outputs, and examples of its use. Note that when you can also open the help documentation for an entire package by typing the package name, e.g., `?dplyr`. This can be very useful to discover what other functions that package has: scroll down to the bottom of any help page and click the "Index" link to see all help pages for that package.

I have prepended each function below with package it comes from so that you know. For example, `summarize` is listed as `dplyr::summarize`. Usually you don't have to do this when using a function, although you can use this to resolve a common bug known as name conflicts (see [this blog post](https://www.r-bloggers.com/2010/08/namespaces-and-name-conflicts/) for discussion). 

# Resources (for this session and others)

- You can find cheatsheets for the dplyr, tidyr, and RMarkdown in the /resources/cheatsheets folder.
- The Open Source textbook R for Data Science (aka, [Wickham's R4DS](https://r4ds.hadley.nz/)) is invaluable. Hadley Wickham is the main developer of the "tidyverse" set of packages, including dplyr, tidyr, ggplot2, stringr, lubridate, and others. See its [section on data transformation](https://r4ds.hadley.nz/data-transform). 
  - The entire second edition of the book is available at [https://r4ds.hadley.nz/](https://r4ds.hadley.nz/).
  - The first edition is also available. It does some things better in my opinion, e.g., it has a better explanation of the pipe (`%>%` or `|>`). See [https://r4ds.had.co.nz/pipes.html](https://r4ds.had.co.nz/pipes.html). 
  - The first edition also talks about RMarkdown, whereas the second edition has moved to a different technology called Quarto (which we won't cover, although they're similar). See [https://r4ds.had.co.nz/r-markdown.html](https://r4ds.had.co.nz/r-markdown.html).
- For those of you who prefer to learn in an interactive environment, I now suggest this web app over Swirl as it is more user-friendly: [https://allisonhorst.shinyapps.io/dplyr-learnr/#section-welcome](https://allisonhorst.shinyapps.io/dplyr-learnr/#section-welcome).
- For those of you who prefer some video content - although seeing other people code can never replace practicing coding yourself! - I can also recommend De Bruine et al.'s Open Source textbook and videos [Data Skills for Reproducible Research](https://psyteachr.github.io/reprores-v3/). E.g., see their page with links to videos for [dplyr](https://psyteachr.github.io/reprores-v3/dplyr.html) and [tidyr](https://psyteachr.github.io/reprores-v3/tidyr.html). 

# Dependencies

The packages these functions come from, which must be loaded to use them.

```{r}

library(readr)
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)
library(janitor)

```

# Basics

## read_csv 

vs. `read.csv()`

*Always, always, always* use relative paths rather than absolute paths.

- Absolute path (bad): "~/Ian/Desktop/my_project/data/data_raw.csv"
- Relative path (good): "../data/data_raw.csv"

When using relative paths: "../" means up one directory level, whereas "directory_name/" goes down one directory level.

Relative paths will work on other machines, where absolute paths will break. 

Relative paths only work in .Rmd files and not .R files. Even without RMarkdown's other benefits, this makes them worth using.

Useful arguments:

- `skip` can be used to skip the first N rows while reading the data

```{r}
# What is the difference between {readr}'s read_csv and base R's read.csv?

?read_csv()

# `read_csv()` is more explicit about what assumption it is making about column types, and prints warning messages about what it has assumed. It is therefore the clearer and more transparent version. 

## Load example data
data_amp_raw <- read_csv(file = "data/raw/data_amp_raw.csv")
data_demographics_raw <- read_csv(file = "data/raw/data_demographics_raw.csv")
data_selfreport_raw <- read_csv(file = "data/raw/data_selfreport_raw.csv")


```

## dir.create

```{r}

# dir.create()

```

## colnames & dput

```{r}

# colnames()

# dput(colnames())

```

# Wrangling

## the pipe: `%>%` or `|>`

`%>%` is the original pipe created for the {magrittr} package and used throughout the tidyverse packages. It is slightly slower but also more flexible. It can handle cases where the input data needs to be passed to a non-first argument or in specific nested ways.

`|>` is a version of the pipe, also called native pipe operator, more recently added to base-R. It is slightly faster but less flexible.  It uses a simple syntax where the result of the left-hand side expression is passed as the first argument to the function on the right-hand side.

**What is the pipe?**
The output of what is left of the pipe is used as the input to the right of the pipe, usually as the first argument or the data argument. The pipe allows us to write code that reads from top to bottom, following a series of steps, in the way that humans organize and describe steps. Without the pipe, code is written from the inside out, in the way that the computer understands it but humans do not as easily.

```{r}

# use a function without the pipe
example_without_pipe <- clean_names(data_demographics_raw)

# use a function with the pipe. the output of what is left of the pipe is used as the input to the right of the pipe, usually as the first argument or the data argument.
example_with_pipe <- data_demographics_raw %>%
  clean_names()

# check they produce identical results
identical(example_without_pipe, example_with_pipe)

```

The utility of this becomes more obvious when there are many steps:

```{r}

# use a series of functions without the pipe
example2_without_pipe <- summarise(group_by(mutate(rename(clean_names(dat = data_amp_raw), unique_id = subject, block = blockcode, trial_type = trialcode, rt = latency), fast_trial = ifelse(rt < 100, 1, 0)), unique_id), percent_fast_trials = mean(fast_trial)*100) 

## the above code reads from the innermost function to the outermost and is way harder to understand and write for us humans.

# use a series of functions with the pipe
example2_with_pipe <- data_amp_raw %>%
  # clean the column names
  clean_names() %>%
  # rename the columns
  rename(unique_id = subject,
         block = blockcode,
         trial_type = trialcode,
         rt = latency) %>%
  # create a new variable using existing ones
  mutate(fast_trial = ifelse(rt < 100, 1, 0)) %>%
  # summarize across trials for each participant
  group_by(unique_id) %>%
  summarise(percent_fast_trials = mean(fast_trial)*100) 

# check they produce identical results
identical(example2_without_pipe, example2_with_pipe)

```


### Nerdy details on the pipe(s)

The {magrittr} `%>%`: 
- is slightly slower due to the additional package overhead, though the difference is generally minimal
- allows flexible positioning through the use of the `.` placeholder. For example, you can write `data %>% f(arg1, .)` to place `data` in a position other than the first argument.
- may consume more memory, especially if the pipeline involves creating many intermediate objects. Each `%>%` call constructs a new environment, which can accumulate if garbage collection doesnâ€™t clear them quickly.
- in larger datasets or for computationally intensive workflows, the greater flexibility calls for a trade-off in processing speed


The native `|>`:
- has less overhead, since it is part of R's core, and can be slightly faster
- error messages and traceback might be more straightforward and easier to debug in some cases.
- tends to use memory more efficiently since it directly pipes the output without added layers of abstraction.
- better for large datasets or computationally intensive workflows, if you need better performance and faster processes


## round_half_up

Did you know that R doesn't use the rounding method most of us are taught in school, where .5 is rounded up to the next integer? Instead it uses "banker's rounding", which is better when you round a very large number of numbers, but worse for reporting the results of specific analyses. 

This is easier to show than explain. What do you expect the output of the below chunk to be? And what is the actual output?

```{r}

round(0.5)
round(1.5)
round(2.5)
round(3.5)
round(4.5)
round(5.5)

```

That is why we need to use `janitor::round_half_up()` in most of our R scripts:

```{r}

round_half_up(0.5)
round_half_up(1.5)
round_half_up(2.5)
round_half_up(3.5)
round_half_up(4.5)
round_half_up(5.5)

```



## clean_names

The function takes and returns a data frame, for ease of piping. It "cleans" the column names of a data frame, such that the names are unique and consist only of the _ character, numbers, and letters. Capitalization preferences can be specified using the `case = ` argument. The default case is "snake" (i.e., this_is_snake_case). To see other types of cases, refer to `?to_any_case()`. 

```{r}

# janitor::clean_names()
?clean_names()

# what are other supported cases?
?to_any_case()


# playing around with clean_names() function...


```

## filter

This function is used on *rows*! You can specify the logical test for filtering in many ways, including equivalence (`==`), negation (`!=`), or membership (`%in%`). It is often better to define what you *do* want (using equivalence or membership) rather than what you *do not* want (negation), as negations are less robust to new data with weird values you didn't think of when you wrote the code. E.g., you could specify `gender != "non-binary"` but this would not catch `non binary`. If you were for example looking to include only men and women, instead use `gender %in% c("man", "woman")`.

You can also have multiple criteria in your filter call, both of which have to be met (x `&` y), or either one of which have to be met (x `|` y).


```{r}

# dplyr::filter()

```


## str_detect

Useful when applying filter to detect specific strings. Example: `str_detect(x, "x_y")` detects the string `"x_y"` in all strings of variable x, no matter where it is positioned. It could be at the beginning, in the middle, or at the end.

```{r}

test <- c("A", "A_", "B")
str_detect(test, "A")
str_detect(test, "_")
str_detect(test, "B")

```

## slice

Slice is for retaining or dropping rows from a df.

We might use it just after reading in files if there are junk rows at the header.

```{r}

# dplyr::slice()

mtcars # this should have 32 rows
slice(mtcars, 2:n()) # this should have 31 rows

```

## select

Used to extract specific columns from a data frame. Also useful for excluding columns (-x, or !c(x,y,z)). 

```{r}

# dplyr::select()


```

## rename

```{r}

# dplyr::rename()

```

## mutate

`mutate()` is used to create new columns or to change the contents of existing ones. The operations inside mutate can range from the very simple, like the above, to much more complex. A single mutate call can also contain multiple mutates.

```{r}

# dplyr::mutate()

```

## case_when

```{r}

# dplyr::case_when() # instead of ifelse()

```

## summarize

It is very common that we need to create summaries across rows. For example, to create the mean and standard deviation of a column like age. This can be done with `summarize()`. Remember: `mutate()` creates new columns or modifies the contents of existing columns, but does not change the number of rows. Whereas `summarize()` reduces a data frame down to one row.

```{r}

# dplyr::summarize() 

```

Like mutate, the operation you do to summarize can also be more complex, such as finding the mean result of a logical test to calculate a proportion. For example, INSERT EXAMPLE WITH MTCARS PACKAGE HERE

```{r}

# dplyr::summarize() 

```

You can also summarize (or indeed mutate) multiple columns in the same way using `across()`, for do-this-across-columns.

```{r}

mtcars %>%
  # ... calculate the mean of every numeric column in the dataset ...
  summarise(across(where(is.numeric), mean, na.rm = TRUE)) %>%
  # ... and then round every column to one decimal place
  mutate(across(everything(), round_half_up, digits = 1))

```


## count

It is very useful to use `count()` to obtain the frequency of each unique value of a given column. 

```{r}

# dplyr::count()

## example use with mtcars...



```

Note that it is also possible to use count to obtain the frequencies of sets of unique values across columns, e.g., unique combinations of item and response. It can be useful to arrange the output by the frequencies.

```{r}

# example with count(x, y) and arrange(desc(n))

```

Also note that `count()` is just the combination of group_by() and summarize() and n()!

```{r}

# add example of this with mtcars dataset...



```

## distinct
Keeps only unique/distinct rows from a data frame. Useful for inspecting the unique values of variables in a large data set.

```{r}

# dplyr::distinct()

```

## group_by

```{r}

# dplyr::group_by() # or ".by =" 

```

## rowwise

```{r}

# dplyr::rowwise()

```

## lead

```{r}

# dplyr::lead()

```

## lag

```{r}

# dplyr::lag()

```

## joins

```{r}

# dplyr::full_join()

# dplyr::left_join()

# dplyr::right_join()

# dplyr::inner_join()

# dplyr::semi_join()

```

## pivots

There are two general types of data shapes: long and wide data. In wide data, every observation has just one row and repeated measurement timepoints for example are displayed as separate columns. This is what makes the data frame wiiiiiiiideeee. In long data, there are multiple rows for each observation and fewer columns. Repeated measurements are displayed as one row, making the data loooooooooong. Depending on the calculations, transformations or analyses you want to conduct, you will need to reshape the data from either wide to long or long to wide format. 

A useful thing to know is that row-math is much faster than column-math in R. So sum scores for example are best calculated with data in long format rather than wide format!

`pivot_wider()`:
- creates one row for each observation
- `names_from =` defines the names for the new columns that will be created; names should come from values in variable `x`
- `values_from =` defines from where the values for the newly created columns should come from; values should come from variable `values`


`pivot_longer()`: 
- creates multiple rows for every observation
- `cols =` which columns should become the "values" for the newly created column (e.g., three different columns named test_trial, pre_trial,  post_trial are put together in a newly created variable called "trial_type")
  - selecting functions such as `starts_with()` can be used for the `cols =` argument
- `names_to =` defines how the newly created column should be named; in the example above, this would be "trial_type"
- `values_to = ` what should the new value column be called? Automatically takes all values in the cells of the specified cols and puts them in the new value column, respective to their observation

```{r}

# tidyr::pivot_longer()
mtcars_long <- mtcars %>% 
  pivot_longer(cols = c(cyl, gear, carb), # columns cylinder, gear and carburetors should be put in...
               names_to = "part", # a new column called "parts"
               values_to = "number_of_parts") # with the values of the former columns now in the new column called "number_of_parts"


# tidyr::pivot_wider()
mtcars_wide_again <- mtcars_long %>% 
  pivot_wider(names_from = part, # back to wide format, with the new-old column names derived from "part"
              values_from = number_of_parts) # and the values derived from "number_of_parts"

```

## drop_na

```{r}

# tidyr::drop_na()

```

## separate

```{r}

# tidyr::separate()

```

## fill

```{r}

# tidyr::fill()

```

# Printing tables

```{r}

# mtcars |> # example data
#   knitr::kable() |> # print a nicer looking table
#   kableExtra::kable_classic(full_width = FALSE) # print nicer again

```

# Other packages 

Other packages you may need for wrangling which aren't covered here:

- library(forcats). Everything to do with factors and factor levels. Useful for plotting and establishing reference levels for statistical tests.
- library(stringr). Everything to do with strings, searching for strings, modifying strings, parsing them, etc.
- library(lubridate). Everything to do with dates, parsing dates, converting date formats, etc. 

# Session info

```{r}

sessionInfo()

```



